name: Scrape DOGE Website

on:
  workflow_dispatch:

jobs:
  scraper:
    runs-on: ubuntu-latest
    permissions:
      contents: write  # Allows write access to repository content (e.g., commit files)
      actions: read  

    services:
      selenium:
        image: selenium/standalone-chrome:latest
        options: --shm-size 2g
        ports:
          - 4444:4444  

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up R
        uses: r-lib/actions/setup-r@v2

      - name: Install packages
        uses: r-lib/actions/setup-r-dependencies@v2
        with:
          packages: |
            any::RSelenium
            any::rvest
            any::purrr

      - name: Use RSelenium to Scrape
        run: |
          library(RSelenium)
          library(rvest)
          library(purrr)
          driver <- remoteDriver(
            remoteServerAddr = "localhost",
            port = 4444,
            browserName = "chrome"
          )
          driver$open()
          driver$navigate("https://doge.gov/savings")
          button <- remote_driver$findElement(using = "xpath", value = "/html/body/div/main/div/div/div[5]/div[2]/div/div/button")
          button$click() 
          Sys.sleep(3)
          page_source <- remote_driver$getPageSource()[[1]]
          page <- LinkExtractor(page_source, ExternalLInks = TRUE)
          fpds <- page$ExternalLinks
          links_df <- data.frame(links = fpds)
          write.csv(links_df, "scraped_links.csv", row.names = FALSE)
          driver$close()
      - name: Commit and Push CSV File to Repository
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@users.noreply.github.com"
          git add ./data/
          git commit -m "Latest data pull" && git push || true
